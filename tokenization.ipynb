{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNl2M8D+pwAxR0afPGZvCmH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahima0811gupta/NLP-PYTHON/blob/main/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnN2bYdNTj7_",
        "outputId": "7703be7c-cd55-4036-84af-c278e8ce40f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Hello,Everyone! This is mahima gupta and this is my github NLP repository.\n",
        "i am learning about tokenization.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ldKTdHUTTtSG"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y56r_lOtUwXk",
        "outputId": "a55032e5-5a50-45c0-df87-859e5d26151c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,Everyone! This is mahima gupta and this is my github NLP repository.\n",
            "i am learning about tokenization.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zZabA9v1VVAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "# paragraph ---->>sentence"
      ],
      "metadata": {
        "id": "G-HXHo8KVcbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdv_gCjXVbBK",
        "outputId": "dcefad44-0b4d-40bd-b8de-b84c17d60176"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "lqZIPPedWy12"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYr1mnjcXCVh",
        "outputId": "fd398fb8-7d87-463a-ce83-a67aa9bb7f16"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,Everyone!',\n",
              " 'This is mahima gupta and this is my github NLP repository.',\n",
              " 'i am learning about tokenization.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "5W0bBmh6XG-z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfgzF1tbXjYa",
        "outputId": "8073e6dc-418e-4011-bc5f-fa3506d48014"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ4zBQ4uXkWj",
        "outputId": "828937fa-8402-4a58-bc5a-37199d3d12aa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,Everyone!\n",
            "This is mahima gupta and this is my github NLP repository.\n",
            "i am learning about tokenization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Paragraph into words\n",
        "#Sentence into  words"
      ],
      "metadata": {
        "id": "KjHDfYWiYN5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "PBdhRKQWYWJK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PllB4YigYz2S",
        "outputId": "1258b2cb-0a90-4768-e5fb-56f8cc29fd07"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Everyone',\n",
              " '!',\n",
              " 'This',\n",
              " 'is',\n",
              " 'mahima',\n",
              " 'gupta',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'my',\n",
              " 'github',\n",
              " 'NLP',\n",
              " 'repository',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'about',\n",
              " 'tokenization',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5E-DK64ZA4V",
        "outputId": "64187b1d-7ad1-4500-e1d7-2c364ff949f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'Everyone', '!']\n",
            "['This', 'is', 'mahima', 'gupta', 'and', 'this', 'is', 'my', 'github', 'NLP', 'repository', '.']\n",
            "['i', 'am', 'learning', 'about', 'tokenization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize  import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "L8GSYaVkaHpj"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsaxMNdGailM",
        "outputId": "4d3dcef6-bf0e-4a86-b703-90e3c3a8b5ed"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Everyone',\n",
              " '!',\n",
              " 'This',\n",
              " 'is',\n",
              " 'mahima',\n",
              " 'gupta',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'my',\n",
              " 'github',\n",
              " 'NLP',\n",
              " 'repository',\n",
              " '.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'about',\n",
              " 'tokenization',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "FkMXgLu6aoNQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "4Cmg-zKnbWrX"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXPiRP3ybfqh",
        "outputId": "2355ed95-4c0b-4daa-f0c7-56602a80a861"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " ',',\n",
              " 'Everyone',\n",
              " '!',\n",
              " 'This',\n",
              " 'is',\n",
              " 'mahima',\n",
              " 'gupta',\n",
              " 'and',\n",
              " 'this',\n",
              " 'is',\n",
              " 'my',\n",
              " 'github',\n",
              " 'NLP',\n",
              " 'repository.',\n",
              " 'i',\n",
              " 'am',\n",
              " 'learning',\n",
              " 'about',\n",
              " 'tokenization',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CJFQCDjUbuaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}